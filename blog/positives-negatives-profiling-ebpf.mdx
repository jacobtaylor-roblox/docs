---
title: The positives and negatives of profiling with eBPF
sidebar_label: Profiling with eBPF
slug: /positives-negatives-profiling-ebpf
date: "2022-09-05"

authors:
  - name: Ryan Perry
    url: https://github.com/Rperry2174
    image_url: https://avatars.githubusercontent.com/u/23323466?v=4
---

## What is eBPF?

At its root, eBPF takes advantage of the kernelâ€™s privileged ability to oversee and control the entire system.
With eBPF you can run sandboxed programs within the operating system and add these to your program at runtime.
To better understand the implications and learn more check out this [blog post](https://ebpf.io/what-is-ebpf)
which goes into much more detail.

![image](https://user-images.githubusercontent.com/23323466/188525916-0e33c64a-9a7a-4fe2-84d6-91111c4d6f39.png)

As you can see in the diagram, some of the most popular use cases for eBPF are related to Networking, Security,
and most relevenat to this blog post -- observability (logs, metrics, traces, and profiles).

## Landscape of eBPF profiling

Over the past few years there has been significant growth in the profiling space as well as the eBPF space and
there are a few notable companies who are innovating at the intersection of profiling and eBPF. Some examples include:

- Pyroscope
- Parca
- Pixie
- Prodfiler (not open source)

While stars are more of a vanity metric, the collective growth is representative of the rapidly growing interest in this space Pyroscope, Pixie, and Parca all gained a significant amount of stars over the same 6 month period.
![image](https://user-images.githubusercontent.com/23323466/188765863-264f9ff3-9d26-4019-8cb3-a26f8535cc7b.png)


It's also worth noting that all the companies listed above, along with many other open source maintainers, OTel community members, and end-users,
have officially proposed for OpenTelemetry to support profiling alongside logs, metrics, and traces. For more information on that or to join the
efforts check out the `#otel-profiling` channel on the CNCF slack!

## Current state of eBPF profiling

There's a wide spectrum of "challenges" with eBPF profiling that can drastically change the value you're able to get from eBPF profiling.
On the "easier" end of that spectrum are fairly standard functionality:

- Collecting data with Low-overhead
- Auto-tagging profiles with available metadata
- One more example

On the moderate "middle of the spectrum is a tricky, but solvable problem

- Finding the best _format_ in which to send profiling data

These are problems that we're hoping to standardize in alignment with the OpenTelemetry vision.

The issues on the "difficult" end of the spectrum are where the various solutions part ways. In particular:

- Dealing with collecting, storing, and applying Symbols
- Finding ways to deal with _lack_ of symbols
- Finding efficinet ways to do other kinds of profiling such as memory profiling
- Finding ways to apply tags dynamically and/or arbitrarily

This blog will focus on Pyroscope's unique approach to solving these issues.

## Pros and cons of eBPF profiling vs non-eBPF profiling

When it comes to modern profiling, it effectively happens at two different levels of the operating system:

- User-space level: Popular profilers like pprof, async-profiler, rbspy, py-spy, pprof-rs, dotnet-trace, etc. 
- Kernel-level: eBPF profilers (Pyroscope, Parca, Pixie, Prodfiler)

The reason that there is this chasm between eBPF profilers and non-eBPF profilers is because at each level 
of the operating system there are positives and negatives of profiling at that particular level.

### Pros/Cons of User-space profiling (non-eBPF)
![image](https://user-images.githubusercontent.com/23323466/188766062-6afb3561-a8d6-439b-8818-b13bd461af9a.png)

### Pros/Cons of Kernel profiling
![image](https://user-images.githubusercontent.com/23323466/188766116-ebee9e9c-a302-4a4b-a856-79e0bb7ef79b.png)

## Pyroscope's solution: Merge User-space and Kernel profiling
<img width="915" alt="image" src="https://user-images.githubusercontent.com/23323466/188767811-a45c0b2c-ec62-4f5e-8fe1-eae3e347e7e0.png">

We believe that there's benefits to _both_ eBPF and non-eBPF profiling and our focus long term is to integrate them together seamlessly in Pyroscope.
You may notice above, the "negatives" of one are the "positives" of the other and the maximum value comes from the fact that using both cancels out the "negatives" while stil benefiting from the "positives". While the tradeoff is that additional overhead is incurred, the overhead for _both_ is so negligible that we believe this to be very net-positive, leaving us with the best of both (profiling) worlds!

Long term, we've charted out two potential paths of either:
- Detecting language via eBPF and using other profilers ccordingly
- Using blablabl to blablablabl (something tolyan said?)

However, in the meantime, we recommend to add eBPF profiling to your cluster _and_
to add a language-specific integrations if you'd like more granular controls and analytic capabilitys for your profiling data.
This is actually how we profile our cloud product's code internally. 
- eBPF for our kubernetes cluster
- ruby gem, pip package, go client, and java client _with tags_ for our language microservices and testing suites
- Lambda extension for our lambda functions

We use eBPF to profile our rideshare example cluster running on kubernetes for our 
Go, Python, Ruby, Java, and Rust examples. Looking at the resulting flamegraph gives a good insight into the
type of information and specificity you get for the user-space profiling results vs. the eBPF results. 

## Examples
### eBPF Java profiling
### user-space Java profiling (pyroscope-java forked from async-profiler)

### eBPF ruby profiling
### user-space Ruby profiling (pyroscope ruby gem forked from rbspy)

import '@pyroscope/flamegraph/dist/index.css';
import Flamegraph from '../src/components/Blog';

<Flamegraph></Flamegraph>
